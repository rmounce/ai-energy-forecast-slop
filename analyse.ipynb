{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "Cell 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Configure Plotly to work in the notebook\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Cell 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "MODEL_TO_ANALYZE = 'price'\n",
    "log_file = f'{MODEL_TO_ANALYZE}_forecast_log.csv'\n",
    "# ---------------------\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(log_file)\n",
    "\n",
    "# --- MODIFICATION: Convert both timestamp columns ---\n",
    "df['forecast_target_time'] = pd.to_datetime(df['forecast_target_time'], format='ISO8601')\n",
    "df['forecast_creation_time'] = pd.to_datetime(df['forecast_creation_time'], format='ISO8601')\n",
    "df.set_index('forecast_target_time', inplace=True)\n",
    "\n",
    "# Drop rows where the actual value hasn't been backfilled yet\n",
    "df.dropna(subset=['actual'], inplace=True)\n",
    "\n",
    "# Calculate the core metric: the error (or \"residual\")\n",
    "df['error'] = df['prediction'] - df['actual']\n",
    "\n",
    "print(f\"Loaded and prepared {len(df)} records for the '{MODEL_TO_ANALYZE}' model.\")\n",
    "df[['forecast_creation_time', 'prediction', 'actual', 'error']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(df['actual'], df['prediction'])\n",
    "rmse = np.sqrt(mean_squared_error(df['actual'], df['prediction']))\n",
    "\n",
    "print(f\"Overall Model Performance ({MODEL_TO_ANALYZE}):\")\n",
    "print(f\"-----------------------------------\")\n",
    "print(f\"Mean Absolute Error (MAE):   {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Cell 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True,\n",
    "                    subplot_titles=(f'{MODEL_TO_ANALYZE.capitalize()} Forecast vs. Actual', 'Prediction Error (Residuals)'),\n",
    "                    vertical_spacing=0.1)\n",
    "\n",
    "# Plot 1: Prediction vs Actual\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df['actual'], name='Actual', mode='lines',\n",
    "                         line=dict(color='blue', width=2)), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df['prediction'], name='Prediction', mode='lines',\n",
    "                         line=dict(color='red', dash='dot', width=2)), row=1, col=1)\n",
    "\n",
    "# Plot 2: Error\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df['error'], name='Error', mode='lines',\n",
    "                         line=dict(color='purple', width=1)), row=2, col=1)\n",
    "fig.add_hline(y=0, line_width=2, line_dash=\"dash\", line_color=\"green\", row=2, col=1)\n",
    "\n",
    "fig.update_layout(height=600, title_text=\"Model Performance Over Time\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Cell 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x='error', nbins=100,\n",
    "                   title=f'Histogram of Prediction Errors for {MODEL_TO_ANALYZE.capitalize()} Model')\n",
    "fig.update_layout(xaxis_title=\"Prediction Error (Prediction - Actual)\", yaxis_title=\"Count\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Cell 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an 'hour' column from the index\n",
    "df['hour'] = df.index.hour\n",
    "\n",
    "# Calculate the average absolute error for each hour\n",
    "hourly_error = df.groupby('hour')['error'].apply(lambda x: np.mean(np.abs(x))).reset_index()\n",
    "\n",
    "fig = px.bar(hourly_error, x='hour', y='error',\n",
    "             title=f'Average Absolute Error by Hour of Day ({MODEL_TO_ANALYZE.capitalize()})')\n",
    "fig.update_layout(xaxis_title=\"Hour of Day\", yaxis_title=\"Mean Absolute Error\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Cell 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by model version and calculate metrics for each\n",
    "performance_by_version = df.groupby('model_version').apply(\n",
    "    lambda g: pd.Series({\n",
    "        'MAE': mean_absolute_error(g['actual'], g['prediction']),\n",
    "        'RMSE': np.sqrt(mean_squared_error(g['actual'], g['prediction'])),\n",
    "        'record_count': len(g)\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "print(\"--- Model Performance by Version ---\")\n",
    "performance_by_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Cell 8: Feature Engineering for Advanced Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Feature Engineering (on the main DataFrame) ---\n",
    "\n",
    "# Calculate the forecast horizon (lead time) for each prediction\n",
    "delta = df.index - df['forecast_creation_time']\n",
    "\n",
    "# Round the forecast horizon UP to the nearest 30-minute interval\n",
    "df['forecast_horizon_hours'] = np.ceil(delta.dt.total_seconds() / 1800) / 2\n",
    "\n",
    "# Extract the time component from the target time (e.g., 14:30:00)\n",
    "df['target_time_of_day'] = df.index.time\n",
    "\n",
    "print(\"Feature engineering complete. 'forecast_horizon_hours' and 'target_time_of_day' are now in the main DataFrame.\")\n",
    "\n",
    "\n",
    "# --- Step 2: Define the SIMPLIFIED Analysis Function ---\n",
    "\n",
    "def analyze_forecast_accuracy(df_prepared, prediction_col, actual_col, analysis_name):\n",
    "    \"\"\"\n",
    "    Generates and displays a full accuracy analysis (metrics and heatmaps).\n",
    "    \n",
    "    NOTE: This function now ASSUMES the input DataFrame already contains\n",
    "    'forecast_horizon_hours' and 'target_time_of_day' columns.\n",
    "    \"\"\"\n",
    "    # Create a working copy, keeping the essential columns for this analysis\n",
    "    cols_to_keep = [prediction_col, actual_col, 'forecast_horizon_hours', 'target_time_of_day']\n",
    "    analysis_df = df_prepared[cols_to_keep].copy()\n",
    "    \n",
    "    analysis_df.dropna(subset=[prediction_col, actual_col], inplace=True)\n",
    "\n",
    "    if analysis_df.empty:\n",
    "        print(f\"\\nNo data available for analysis: {analysis_name}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # --- Metrics ---\n",
    "    mae = mean_absolute_error(analysis_df[actual_col], analysis_df[prediction_col])\n",
    "    rmse = np.sqrt(mean_squared_error(analysis_df[actual_col], analysis_df[prediction_col]))\n",
    "    print(f\"\\n--- Performance Analysis for: {analysis_name} ---\")\n",
    "    print(f\"MAE: {mae:.4f} | RMSE: {rmse:.4f}\")\n",
    "\n",
    "    # --- Error Calculation (the only feature it creates itself) ---\n",
    "    analysis_df['error'] = analysis_df[prediction_col] - analysis_df[actual_col]\n",
    "    \n",
    "    # --- Heatmap 1: Error Magnitude ---\n",
    "    heatmap_mae = pd.pivot_table(analysis_df, values='error', index='target_time_of_day',\n",
    "                                 columns='forecast_horizon_hours', aggfunc=lambda x: np.mean(np.abs(x)))\n",
    "    fig_mae = px.imshow(heatmap_mae, labels=dict(x=\"Forecast Horizon (Hours)\", y=\"Time of Day\", color=\"MAE\"),\n",
    "                        title=f\"<b>Error Magnitude (MAE) for {analysis_name}</b>\", aspect='auto')\n",
    "    fig_mae.update_yaxes(tickformat='%H:%M', autorange=\"reversed\")\n",
    "    fig_mae.show()\n",
    "    \n",
    "    # --- Heatmap 2: Error Bias ---\n",
    "    heatmap_bias = pd.pivot_table(analysis_df, values='error', index='target_time_of_day',\n",
    "                                  columns='forecast_horizon_hours', aggfunc=np.mean)\n",
    "    fig_bias = px.imshow(heatmap_bias, labels=dict(x=\"Forecast Horizon (Hours)\", y=\"Time of Day\", color=\"Mean Error\"),\n",
    "                         title=f\"<b>Error Bias (Mean Error) for {analysis_name}</b>\", aspect='auto',\n",
    "                         color_continuous_scale='RdBu_r', color_continuous_midpoint=0)\n",
    "    fig_bias.update_yaxes(tickformat='%H:%M', autorange=\"reversed\")\n",
    "    fig_bias.show()\n",
    "\n",
    "print(\"Analysis function is now defined and ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Cell 9: Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run Analysis for the Main Model ---\n",
    "# The main 'df' DataFrame now has all the required columns from Cell 8.\n",
    "analyze_forecast_accuracy(df, 'prediction', 'actual', f\"{MODEL_TO_ANALYZE.capitalize()} Model\")\n",
    "\n",
    "# --- Run Analysis for the Input Covariates ---\n",
    "analyze_forecast_accuracy(df, 'power_pv', 'power_pv_actual', \"Solcast PV Forecast\")\n",
    "analyze_forecast_accuracy(df, 'temperature_adelaide', 'temperature_adelaide_actual', \"BOM Temperature Forecast\")\n",
    "analyze_forecast_accuracy(df, 'humidity_adelaide', 'humidity_adelaide_actual', \"BOM Humidity Forecast\")\n",
    "analyze_forecast_accuracy(df, 'wind_speed_adelaide', 'wind_speed_adelaide_actual', \"BOM Wind Speed Forecast\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-energy-forecast-slop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
